{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca56ad26",
   "metadata": {},
   "source": [
    "# Table of contents\n",
    "*Aoife Gregg, June 2023*\n",
    "1. [Introduction](#introduction)\n",
    "2. [Importing and processing data](#data)\n",
    "    1. [Importing data from AWS database](#AWS)\n",
    "    2. [Processing data](#processing_data)\n",
    "3. [Email anonymisation](#anonymisation)\n",
    "4. [Large language models for email analysis](#LLMs)\n",
    "    1. [Open-source LLM analysis of emails](#open_source_LLM)\n",
    "    2. [OpenAI LLM analysis of emails](#openai_LLM)\n",
    "    3. [Comparison of LLM results](#LLM_comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9bcfea",
   "metadata": {},
   "source": [
    "## Introduction <a name=\"introduction\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ed68a3",
   "metadata": {},
   "source": [
    "This notebook goes through the code used for the Faculty X LocalGlobe project in 2023, including graphs of key results.\n",
    "\n",
    "The key output of this project was the demonstration of extracting key financial data from investor update emails using a range of large language models. OpenAI gpt-3.5-turbo performed best out of the models tested, but tests were limited due to legal and data security concerns. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c3cc1d",
   "metadata": {},
   "source": [
    "Run this cell to install the relevant packages for this notebook. It assumes a Linux or conda installation (for PyTorch, needed for HuggingFace transformers). I recommend installing and running this code in a virtual environment to avoid version incompatibilities in your system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307a8dda",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd74d76a",
   "metadata": {},
   "source": [
    "Importing relevant packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fa3e8f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine,text\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import plotly\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import re\n",
    "import html\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, AutoModelForCausalLM, pipeline, logging\n",
    "import spacy\n",
    "import ast\n",
    "from sklearn.metrics import accuracy_score,mean_absolute_error\n",
    "import kaleido"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f49ab46",
   "metadata": {},
   "source": [
    "## Importing and processing data<a name=\"data\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d035f427",
   "metadata": {},
   "source": [
    "### Importing data from AWS database<a name=\"AWS\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39643a56",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "alchemyEngine = create_engine(\n",
    "    os.getenv(\"DATABASE_URL\"),\n",
    "    pool_recycle=3600\n",
    ")\n",
    "db_connection = alchemyEngine.connect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582c0326",
   "metadata": {},
   "source": [
    "Run the appropriate cells below to download the data of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662e7d1f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# trello_card = all information contained in Trello cards\n",
    "sql_query = text(\"select * from nazare.ingestion.trello_card\") \n",
    "df_trello = pd.read_sql(sql_query, db_connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ec2bd3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# tf_form = typeform survey results\n",
    "sql_query = text(\"select * from nazare.ingestion.tf_form\")\n",
    "\n",
    "df_tf = pd.read_sql(sql_query, db_connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8df78c4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#dealroom company data\n",
    "sql_query = text(\"select * from nazare.ingestion.dealroom_company\")\n",
    "\n",
    "df_dealroom_company = pd.read_sql(sql_query, db_connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06391351",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#dealroom funding round data\n",
    "sql_query_dealroom_fundinground = text(\"select * from nazare.ingestion.dealroom_fundinground\")\n",
    "\n",
    "df_dealroom_fundinground = pd.read_sql(sql_query_dealroom_fundinground, db_connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390ebb1e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ops_companyfund = data on individual portfolio companies from Google sheet\n",
    "sql_query = text(\"select * from nazare.ingestion.ops_companyfund\")\n",
    "\n",
    "df_ops = pd.read_sql(sql_query, db_connection)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec90b687",
   "metadata": {},
   "source": [
    "### Processing data<a name=\"processing_data\"></a>\n",
    "\n",
    "#### Email data\n",
    "\n",
    "Collecting all text comments for all companies from Trello cards, some of which are emails uploaded as comments. Not filtering between emails and other comments as the LLM should be able to cope with the different formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ad0e02",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trello_ids = df_trello['short_link_hash'].unique()\n",
    "\n",
    "# create a dataframe containing the comments on the company along with identifying information\n",
    "\n",
    "data = []\n",
    "\n",
    "for trello_id, group in df_trello.groupby(by='short_link_hash'):\n",
    "    \n",
    "    # There should only be one company in each group; if not, there has been a problem with labelling in the dataframe    \n",
    "    assert len(group) == 1, '''\n",
    "    There is more than one card in the database with this Trello ID, which should not happen. \n",
    "    Resolve this issue before continuing.\n",
    "    '''    \n",
    "    \n",
    "    row = group.iloc[0,:]\n",
    "    \n",
    "    comments = row['comments']\n",
    "    \n",
    "    for comment in comments:\n",
    "    \n",
    "        data.append([\n",
    "            row['name'],\n",
    "            trello_id,\n",
    "            comment['date'][:10],\n",
    "            comment['id'],\n",
    "            comment['data']['text'],\n",
    "        ])\n",
    "        \n",
    "df_comments = pd.DataFrame(\n",
    "    data=data,\n",
    "    columns = ['company_name','trello_id','date','comment_id','text']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f21e18f",
   "metadata": {},
   "source": [
    "#### Typeform survey data\n",
    "\n",
    "Restructuring Typeform survey data to extract results from json to pandas dataframe.\n",
    "\n",
    "First, removing surveys that were sent out but no-one responded to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e537a960",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_only_answered_surveys = df_tf.loc[df_tf.responses.str.len()>0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4557a3a4",
   "metadata": {},
   "source": [
    "Defining a function to extract survey answers from a json format to a dictionary with the question as keys and the answers as values. \n",
    "\n",
    "The json structure varies depending on the question in the survey due to the way it is exported by Typeform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde2c5b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def extract_survey_data(fields, response):\n",
    "    '''Extracts survey answers from json format to a dictionary with the question as keys and answer as values.'''\n",
    "\n",
    "    questions = {question[\"ref\"]:question[\"title\"] for question in fields}\n",
    "    \n",
    "    answers = {}\n",
    "\n",
    "    for answer in response[\"answers\"]:\n",
    "        \n",
    "        question = questions[answer[\"field\"][\"ref\"]]\n",
    "        \n",
    "        if answer[\"type\"] == \"choices\": #collects multiple choice where multiple answers are allowed (question: concerns)\n",
    "            if \"labels\" in answer[\"choices\"]:\n",
    "                extracted_answer  = answer[\"choices\"][\"labels\"]\n",
    "            elif \"other\" in answer[\"choices\"]:\n",
    "                extracted_answer = answer[\"choices\"][\"other\"]\n",
    "            else:\n",
    "                raise RunTimeError(f\"Answer type {answer['type']} Did not have recognised options: {answer['choices']}. Question was {question}\")\n",
    "                \n",
    "        else:\n",
    "            \n",
    "            match answer[\"field\"][\"type\"]:\n",
    "                \n",
    "                # 'dropdown' collects Meeting Type\n",
    "                # 'short_text' collects text comments on Excitement, Vision, Market, Product, Team, Timing, Fundraising, Opportunity, Return\n",
    "                case \"dropdown\" | \"short_text\": \n",
    "                    extracted_answer = answer[\"text\"]\n",
    "                    \n",
    "                # 'opinion' scale collects scales on Excitement, Vision, Market, Product, Team, Timing, Fundraising, Fit, Opportunity    \n",
    "                # 'rating' collects opinion numbers on another format of Excitement question\n",
    "                case \"opinion_scale\" | \"rating\": \n",
    "                    extracted_answer = answer[\"number\"]\n",
    "                                               \n",
    "                case \"multiple_choice\":\n",
    "                    if answer[\"choice\"][\"id\"] != \"other\":\n",
    "                        extracted_answer  = answer[\"choice\"][\"label\"] #collects Return and name of person filling out survey\n",
    "                    else:\n",
    "                        extracted_answer  = answer[\"choice\"][\"other\"] # collects name of person filling out survey if they are not in the system                              \n",
    "                                               \n",
    "                case _:\n",
    "                    raise RunTimeError(f\"Answer {answer} not recognised. Question was {question}\")\n",
    "        \n",
    "        # The exact formatting of the questions varies between surveys, so this section uses knowledge of keywords \n",
    "        # in the questions to sort all questions into the correct categories with consistent naming.\n",
    "        \n",
    "        question_categories = [\n",
    "            'Excitement',\n",
    "            'Vision',\n",
    "            'Opportunity',\n",
    "            'Team',\n",
    "            'Fundraising',\n",
    "            'Fit',\n",
    "            'Product',\n",
    "            'Timing',\n",
    "        ]\n",
    "        \n",
    "        for category in question_categories:\n",
    "            if category.lower() in question.lower():\n",
    "                if type(extracted_answer) is int:\n",
    "                    question = f'{category} score'\n",
    "                elif type(extracted_answer) is str:\n",
    "                    question = f'{category} text response'\n",
    "                    \n",
    "        # these questions have slightly different formatting\n",
    "        if 'market' in question.lower() and 'timing' not in question.lower():\n",
    "            if type(extracted_answer) is int:\n",
    "                question = 'Market score'\n",
    "            elif type(extracted_answer) is str:\n",
    "                question = 'Market text response'\n",
    "\n",
    "        if 'return' in question.lower():\n",
    "            if 'because' not in question.lower():\n",
    "                question = 'Return value'\n",
    "            else:\n",
    "                question = 'Return text response'\n",
    "\n",
    "        if 'name' in question.lower() and 'company' not in question.lower():\n",
    "            question = 'Responder name'\n",
    "\n",
    "        if 'where are we' in question.lower():\n",
    "            question = 'Meeting stage'\n",
    "\n",
    "        if 'concern' in question.lower():\n",
    "            question = 'Concerns'\n",
    "                                \n",
    "        answers[question] = extracted_answer\n",
    "                                               \n",
    "    return answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12834bb",
   "metadata": {},
   "source": [
    "Using the above function to cycle through all surveys in the AWS database and store the results in a pandas dataframe. \n",
    "\n",
    "Then sorting to ensure accuracy when plotting data over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4ba9f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = [] \n",
    "for idx,row in df_only_answered_surveys.iterrows():\n",
    "    for response in row['responses']:\n",
    "        \n",
    "        answer = extract_survey_data(row['fields'], response)\n",
    "        \n",
    "        answer.update(row[['company_name','trello_id','created_at','id']].to_dict())\n",
    "        \n",
    "        data.append(answer)\n",
    "        \n",
    "df_extended_responses = pd.DataFrame(data)\n",
    "\n",
    "df_extended_responses.sort_values(by=['created_at'],inplace=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1589d7ad",
   "metadata": {},
   "source": [
    "#### Dealroom data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99a6a79",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# converting time data to pandas datetime type\n",
    "df_dealroom_fundinground[\"day\"] = 1 # pandas datetime requires year, month, and day, but dealroom data doesn't include day. Setting the day to the 1st of the month.\n",
    "df_dealroom_fundinground[\"time\"] = pd.to_datetime(df_dealroom_fundinground[[\"year\",\"month\",\"day\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff9557c",
   "metadata": {},
   "source": [
    "Choose the company of interest and check that it appears in the dealroom database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca72ddab",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "company_name = 'Example'\n",
    "mask = df_dealroom_company[\"name\"] == company_name\n",
    "\n",
    "company_id = df_dealroom_company[mask][\"id\"].iloc[0]\n",
    "df_dealroom_company[mask].head() #check that you're looking at the correct company/the company is mentioned in dealroom data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bdab5b1",
   "metadata": {},
   "source": [
    "Plot the funding of the company over time, labelled with the funding round type. On a secondary axis, plot the survey scores over time. \n",
    "\n",
    "This graph is useful for seeing how opinions over time may be related to the funding a company is receiving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b496b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "\n",
    "# creating a filter to extract only the data related to the chosen company\n",
    "company_id_mask = df_dealroom_fundinground[\"company_id\"] == company_id\n",
    "\n",
    "# plotting the funding over time\n",
    "fig.add_trace(go.Scatter(x=df_dealroom_fundinground[company_id_mask]['time'],\n",
    "                         y=df_dealroom_fundinground[company_id_mask]['amount_usd_million'],\n",
    "                         mode='markers',\n",
    "                         marker_symbol = 'x-thin',\n",
    "                         marker_line_width=1,\n",
    "                         marker_size = 10,\n",
    "                         text=df_dealroom_fundinground[company_id_mask]['round'],\n",
    "                         name = 'Funding raised'\n",
    "                        ),\n",
    "              secondary_y=False,\n",
    "              )\n",
    "\n",
    "# plotting the survey scores over time\n",
    "numeric_columns = [\n",
    "    'Excitement score',\n",
    "    'Fit score',\n",
    "    'Fundraising score',\n",
    "    'Market score',\n",
    "    'Opportunity score',\n",
    "    'Product score',\n",
    "    'Team score',\n",
    "    'Timing score',\n",
    "    'Vision score',\n",
    "]\n",
    "\n",
    "for column in numeric_columns:\n",
    "    fig.add_trace(go.Scatter(x=df_extended_responses[df_extended_responses['company_name']== company_name]['created_at'].unique(),\n",
    "                             y=df_extended_responses[df_extended_responses['company_name']== company_name].groupby('created_at')[column].mean(),\n",
    "                             name = column\n",
    "                            ),\n",
    "                  secondary_y=True,\n",
    "                  )\n",
    "\n",
    "\n",
    "fig.update_layout(\n",
    "    title_text=company_name\n",
    ")\n",
    "\n",
    "\n",
    "fig.update_xaxes(title_text=\"Time\")\n",
    "fig.update_yaxes(title_text=\"Funding raised (million USD)\", secondary_y=False)\n",
    "fig.update_yaxes(title_text=\"Survey score\", secondary_y=True)\n",
    "\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d79c53",
   "metadata": {},
   "source": [
    "The following section demos some ways in which existing internal data can be visualised. \n",
    "\n",
    "*Anonymisation: I have removed explanations of exactly what I am plotting here in order to preserve privacy.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33864adb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_trello['trello_id'] = df_trello['short_link_hash']\n",
    "df_merged = df_ops.merge(df_tf,how='left',on='trello_id')\n",
    "df_merged = df_merged.merge(df_trello, how = 'left', on = 'trello_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e9db6d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def replace_blank(value,fill='unknown'):\n",
    "    if str(value) == '' or str(value) == ' ':\n",
    "        output = fill\n",
    "    else:\n",
    "        output = value\n",
    "    return output\n",
    "\n",
    "df_merged['braggy'] = df_merged['braggy'].map(replace_blank)\n",
    "\n",
    "df_merged.replace('', np.nan, inplace=True)\n",
    "df_merged.dropna(subset=['company_name','initial_stage','braggy'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5f0232",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "x_axis = 'initial_investment'\n",
    "y_axis = 'current_post_valuation'\n",
    "\n",
    "\n",
    "for value in df_merged.investment_type.unique():\n",
    "    df_temp = df_merged[df_merged['investment_type'] == value]\n",
    "    fig.add_trace(go.Scatter(x=df_temp[x_axis],\n",
    "                             y=df_temp[y_axis],\n",
    "                             mode='markers',\n",
    "                             text=df_temp[\"company_name\"]+', '+ df_temp[\"initial_stage\"],\n",
    "                             name = value,\n",
    "                             marker_line_width=1,\n",
    "                             marker_symbol = 'circle-open',\n",
    "                            )\n",
    "                  )\n",
    "\n",
    "fig.update_layout(\n",
    "    xaxis_title=x_axis,\n",
    "    yaxis_title=y_axis,\n",
    ")\n",
    "\n",
    "\n",
    "fig2 = go.Figure()\n",
    "x_axis = 'current_owned'\n",
    "y_axis = 'current_nav'\n",
    "\n",
    "for value in df_merged.braggy.unique():\n",
    "    df_temp = df_merged[df_merged['braggy'] == value]\n",
    "    fig2.add_trace(go.Scatter(x=df_temp[x_axis],\n",
    "                             y=df_temp[y_axis],\n",
    "                             mode='markers',\n",
    "                             text=df_temp[\"company_name\"]+', '+ df_temp[\"initial_stage\"],\n",
    "                             name = value,\n",
    "                             marker_line_width=1,\n",
    "                             marker_symbol = 'circle-open',\n",
    "                            )\n",
    "                  )\n",
    "\n",
    "\n",
    "fig2.update_layout(\n",
    "    xaxis_title=x_axis,\n",
    "    yaxis_title=y_axis,\n",
    ")\n",
    "\n",
    "\n",
    "fig3 = go.Figure()\n",
    "x_axis = 'current_owned'\n",
    "y_axis = 'current_nav'\n",
    "\n",
    "fig3.add_trace(go.Scatter(x=df_merged[x_axis],\n",
    "                         y=df_merged[y_axis],\n",
    "                         mode='markers',\n",
    "                         text=df_merged[\"company_name\"]+', '+ df_merged[\"initial_stage\"],\n",
    "                         marker_line_width=1,\n",
    "                         marker_symbol = 'circle-open',\n",
    "                         marker_color = df_merged.latest_excitement_score,\n",
    "                         marker = dict(\n",
    "                            colorbar=dict(\n",
    "                                title=\"Latest Excitement\"\n",
    "                         )),\n",
    "                         \n",
    "                        )\n",
    "              )\n",
    "\n",
    "fig3.update_layout(\n",
    "    xaxis_title=x_axis,\n",
    "    yaxis_title=y_axis,\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "fig2.show()\n",
    "fig3.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a055d23",
   "metadata": {},
   "source": [
    "## Email anonymisation<a name=\"anonymisation\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d780a12d",
   "metadata": {},
   "source": [
    "Defining an anonymisation function that takes text and a list of companies and people names. Removes those names, along with a list of LocalGlobe portfolio company names, from the input text. Outputs anonymised text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8a2fcf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def anonymiser(text, people = None, organisations = None):\n",
    "    \n",
    "    '''\n",
    "    Takes text and optionally a list of people and companies.\n",
    "    Combines these lists with a list of all portfolio companies in LocalGlobe \n",
    "    (extracted from company names in the Typeform survey database).\n",
    "    Removes any words with length >2 that appears in these lists from the input text\n",
    "    and replaces it with XXX or similar.\n",
    "    Returns this anonymised text.\n",
    "    '''\n",
    "    \n",
    "    # use regex to detect  an URL in the form of 'http(s)...' and replace with [URL]\n",
    "    anonymised_text = re.sub(r'http\\S+', '[URL]', text)\n",
    "    # use regex to detect  an email in the form of '[...]@[...].[...]' and replace with [XXX@XXXX.XX]\n",
    "    anonymised_text = re.sub(r'[A-Za-z0-9]*@[A-Za-z]*\\.?[A-Za-z0-9]*', \"XXX@XXXX.XX\", anonymised_text)\n",
    "    \n",
    "    \n",
    "    # extracts list of portfolio companies from Typeform survey data\n",
    "    if organisations is None:\n",
    "        organisations = df_tf[\"company_name\"].tolist()\n",
    "    else:\n",
    "        organisations += df_tf[\"company_name\"].tolist()\n",
    "    organisations += ['LocalGlobe']\n",
    "    \n",
    "    if people is None:\n",
    "        people = []\n",
    "\n",
    "    # removes brackets from company and people names to avoid parsing errors\n",
    "    for count, value in enumerate(organisations):\n",
    "        organisations[count] = re.sub(r\"[\\(\\)\\{\\}\\[\\]]\", \" \", value)\n",
    "    for count, value in enumerate(people):\n",
    "        people[count] = re.sub(r\"[\\(\\)\\{\\}\\[\\]]\", \" \", value)\n",
    "\n",
    "    # substituting people's names in the text with 'XXX '\n",
    "    if len(people)>0:\n",
    "        for person in people:\n",
    "            for split in person.split(' '):\n",
    "                # only add names of length >2 to the list as shorter ones remove too much text during the anonymisation process\n",
    "                if len(split)>2: \n",
    "                    split_escaped = re.escape(split) # escaping non-alphanumeric characters\n",
    "                    # use regex to substitute all names in the input text with 'XXX '\n",
    "                    anonymised_text = re.sub(rf'(?:^|\\W){split_escaped}(?:$|\\W|$)', ' XXX ',anonymised_text,flags=re.IGNORECASE)\n",
    "\n",
    "    # substituting company names in the text with 'XXX '\n",
    "    if len(organisations)>0:\n",
    "        for organisation in organisations:\n",
    "            organisation_escaped = re.escape(organisation)\n",
    "            anonymised_text = re.sub(rf'(?:^|\\W){organisation_escaped}(?:$|\\W|$)', ' XXX ',anonymised_text,flags=re.IGNORECASE)\n",
    "\n",
    "\n",
    "    return anonymised_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ba8811",
   "metadata": {},
   "source": [
    "Downloading the English language model used by spaCy for NER. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1ba2eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e21ab26",
   "metadata": {},
   "source": [
    "Setting up the BERT and spaCy NER models. Both BERT and spaCy methods are used as BERT generally performs better, but sometimes misses out people's names. Adding in an additional spaCy process (which is generally quick) helps to pick up those missed names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56731729",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Using HuggingFace transformers to access BERT model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-large-finetuned-conll03-english\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"xlm-roberta-large-finetuned-conll03-english\")\n",
    "nlp = pipeline(\"ner\", model = model, tokenizer=tokenizer,aggregation_strategy = \"simple\")\n",
    "\n",
    "nlp_spacy = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9edb0ef2",
   "metadata": {},
   "source": [
    "The following cells use text from **Trello comments** for a given company and use BERT and spaCy for named entity recognition to read through text and identify company and people names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b45086",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "company_name = 'example' # set the company of interest \n",
    "company_trello_id = df_tf[df_tf['company_name'].str.contains(company_name)]['trello_id'].iloc[0] # find the company Trello ID from its name\n",
    "\n",
    "df_comments_company = df_comments[df_comments['trello_id']==company_trello_id]\n",
    "\n",
    "orgs = []\n",
    "people = []\n",
    "\n",
    "\n",
    "# for each comment in the dataframe, extract the text and run BERT NER to identify company and people names, then append them to a list.\n",
    "# we are processing each comment individually as this improves NER performance\n",
    "for comment_text in df_comments_company['text']:\n",
    "    ner_results = nlp(comment_text) # ner_results contains list of all 'named entities' identified by BERT NER\n",
    "    for result in ner_results:\n",
    "        if result['entity_group'] == 'PER': # if the named entity is a person\n",
    "            if result['word'] not in people:\n",
    "                people.append(result['word'])\n",
    "        if result['entity_group'] == 'ORG': # if the named entity is a company\n",
    "            if result['word'] not in orgs:\n",
    "                orgs.append(result['word'])\n",
    "\n",
    "    \n",
    "# Read all comments into one text variable for spaCy analysis and then anonymisation.\n",
    "text_data = \"\\n\".join(df_comments_company['text'])\n",
    "\n",
    "\n",
    "# Repeat the NER process with spaCy NER to pick up more people's names and add them to the list of people\n",
    "doc = nlp_spacy(text_data)\n",
    "\n",
    "types = [\"PERSON\"]\n",
    "for token in doc:\n",
    "    if token.ent_type_ in types and token.text not in people:\n",
    "        people.append(token.text)\n",
    "\n",
    "# Input the collected people's and company names into the anonymisation function and print the anonymised text.\n",
    "anonymised_text = anonymiser(text_data,people=people,organisations=orgs)\n",
    "print(anonymised_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8126dfaa",
   "metadata": {},
   "source": [
    "## Large language models for email analysis<a name=\"LLMs\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8029c33",
   "metadata": {},
   "source": [
    "### Open-source LLM analysis of emails<a name=\"open_source_LLM\"></a>\n",
    "\n",
    "The open-source LLM models are tested using HuggingFace transformers. First, choose the model to use by running one of the below cells or adding in another HF model (https://huggingface.co/models).\n",
    "\n",
    "If you're using a **quantised** model (here, these are the models which use AutoGPTQ), first install and import the required packages. These packages require CUDA, so check/install this beforehand. Again, I recommend that you run this code in a virtual environment to avoid introducing version incompatibilities to your system. There is no need to install/import these if you are not using a quantized model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc575397",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install -r requirements_quantised.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6450b32e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0565ed4a",
   "metadata": {},
   "source": [
    "A quantised version of **Falcon 7B** is small enough to run on a local computer, but does not produce consistent results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feee7edd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "MODEL = \"TheBloke/falcon-7b-instruct-GPTQ\"\n",
    "\n",
    "model_basename = \"gptq_model-4bit-64g\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL, use_fast=True)\n",
    "\n",
    "model = AutoGPTQForCausalLM.from_quantized(\n",
    "        MODEL,\n",
    "        model_basename=model_basename,\n",
    "        device=\"cuda:0\",                                        \n",
    "        use_safetensors=True,\n",
    "        use_triton=False,\n",
    "        trust_remote_code=True,\n",
    "        quantize_config=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5be381",
   "metadata": {},
   "source": [
    "**RedPajama** requires roughly >20 GB VRAM to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8fc17a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "MODEL = \"togethercomputer/RedPajama-INCITE-7B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL, use_fast=True)\n",
    "\n",
    "model = AutoModelForCausalLM.from_quantized(\n",
    "    MODEL, \n",
    "    device=\"cuda:0\", \n",
    "    use_safetensors=True, \n",
    "    use_triton=False, \n",
    "    trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d22d2b",
   "metadata": {},
   "source": [
    "**Vicuna 33B** provides better results than most other open-source models tested, but takes a lot of memory to run. Here, a quantised version (i.e. modified to run using less memory) is used but this still requires a GPU with >25GB VRAM.\n",
    "\n",
    "Typical errors: Vicuna 33B cannot reliably distinguish between similar metrics e.g. monthly total revenue vs monthly subscription revenue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2185cbde",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "MODEL = \"TheBloke/vicuna-33B-GPTQ\"\n",
    "model_basename = \"vicuna-33b-GPTQ-4bit--1g.act.order\"\n",
    "\n",
    "model = AutoGPTQForCausalLM.from_quantized(\n",
    "    MODEL,\n",
    "    model_basename=model_basename,\n",
    "    device=\"cuda:0\",\n",
    "    use_safetensors=True,\n",
    "    use_triton=False,\n",
    "    trust_remote_code=True,\n",
    "    quantize_config=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee9270f",
   "metadata": {},
   "source": [
    "**Falcon 40B** provides some of the best results of the open-source models tested, but takes a lot of memory to run. Here, a quantised version (i.e. modified to run using less memory) is used but this still requires a GPU with >40GB VRAM.\n",
    "\n",
    "Typical errors: Falcon 40B is prone to hallucination, so do not trust it on documents where the answer may not be present.\n",
    "\n",
    "*Note (August 2023): Since working on this project, Meta has released a new open-source LLM (Llama 2, which has 70B parameters and Meta claims outperforms GPT-3.5) that may perform better than Falcon 40B.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad73ec1b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "MODEL = \"TheBloke/falcon-40b-instruct-GPTQ\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL, use_fast=True)\n",
    "\n",
    "model = AutoGPTQForCausalLM.from_quantized(\n",
    "    MODEL, \n",
    "    device=\"cuda:0\", \n",
    "    use_safetensors=True, \n",
    "    use_triton=False, \n",
    "    trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262cef19",
   "metadata": {},
   "source": [
    "Next, define a function to extract information from body of text using the selected model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a94dedb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def extract_information(model_choice,tokenizer_choice,question, document,end_token_length = 0):\n",
    "    '''\n",
    "    Run a pre-defined model and tokenizer on input document text.\n",
    "    If end_token_length is provided, it will cut off that many characters from the end of the \n",
    "    response given by the model.\n",
    "    '''\n",
    "    \n",
    "    prompt = f\"\"\"User: {question} \\n Context: {document} \\n \\n Assistant:\\n \"\"\"\n",
    "\n",
    "    input_ids = tokenizer_choice(prompt, return_tensors='pt').input_ids.cuda()\n",
    "    output = model_choice.generate(inputs=input_ids, temperature=0, max_new_tokens=512)\n",
    "    full_output_text = tokenizer.decode(output[0])\n",
    "    input_length = len(tokenizer.decode(input_ids[0]))\n",
    "    results_output_text = full_output_text[input_length:-end_token_length] \n",
    "    torch.cuda.empty_cache()\n",
    "    return results_output_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab30b5a8",
   "metadata": {},
   "source": [
    "Choose which company's comments you're looking at.\n",
    "\n",
    "First, check the Trello ID of the company you're looking for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa8299e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "company_name = 'example' # set the company of interest \n",
    "name_mask = df_comments[\"company_name\"].str.contains(company_name)\n",
    "df_comments[name_mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0561aa8",
   "metadata": {},
   "source": [
    "Due to multiple Trello boards containing the same company names, check which board is actually referring to the company you want. If the correct company is at the top of the results above, use the automatic company ID below. If not, either manually set it or set the *iloc* index to the correct value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4814239",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "company_trello_id = df_comments[name_mask]['trello_id'].iloc[0] # set the company Trello ID from its name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8717ee70",
   "metadata": {},
   "source": [
    "Define the question to run on these comments and run the selected model on each comment one at a time, storing these results as text in a json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73023bda",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "question = \"\"\"You are an intelligent assistant answering questions based on information from documents.\n",
    "Answer the following questions using the context given.\n",
    " 1. What is the total monthly recurring revenue? \\n\n",
    " 2. What is the runway in months? \\n\n",
    " 3. What is the total working capital? \\n\n",
    " 4. What is the total number of full time employees? \\n\n",
    " 5. What is the monthly burn rate? \\n\n",
    " 6. On what date was this document written? \\n\n",
    " 7. What is the total annual recurring revenue? \n",
    " \"\"\"    \n",
    "\n",
    "results = [\n",
    "    {\n",
    "        'model': model,\n",
    "        'question' : question,    \n",
    "    }\n",
    "]\n",
    "\n",
    "for idx, row in df_comments[df_comments['trello_id']==company_trello_id].iterrows():\n",
    "    answers = extract_information(model,tokenizer,question,row['text'])\n",
    "    \n",
    "    result = {\n",
    "        'answers' : answers,\n",
    "    }\n",
    "    \n",
    "    # storing the metadata along with the answers\n",
    "    # note the date here refers to the date from the Trello comment metadata, not the date the email was written\n",
    "    result.update(row[['date','trello_id','comment_id']].to_dict())\n",
    "    \n",
    "    results.append(result)\n",
    "\n",
    "with open(f\"{company_name}.json\", \"w\") as file:\n",
    "    json.dump(results, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99517e0",
   "metadata": {},
   "source": [
    "The text answers then need to be processed in order to plot them. The formatting varies significantly for different models used, but we can expect some consistency if the questions are asked in the form of a numbered list: most models will then also output a numbered list, which can be processed by detecting the numbers and associating them with the correct question. \n",
    "\n",
    "I have not yet seen a model that will correctly answer the questions but incorrectly number the answers, so this processing method should not lose any useful information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1664654e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(f'{company_name}.json','r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "data = data[1:]  # remove the element containing information about the run\n",
    "\n",
    "extracted_data = []\n",
    "params = ['monthly_revenue', 'runway', 'capital', 'FTEs', 'burn_rate', 'date','annual_revenue'] \n",
    "\n",
    "# iterating over the json containing the LLM results for the set of emails for a given company\n",
    "for element in data:\n",
    "    text = element['answers']\n",
    "    extracted_strings = {}\n",
    "\n",
    "    #I know the format of my text is a string in the form of a numbered list from 1. to 7. \n",
    "    #This section splits the string into each bullet point.\n",
    "    text = re.sub(',','',text) #removing commas to interpret numbers more easily: £100,000 -> £100000\n",
    "    split_text = re.split(r'(\\s|^)\\d\\.\\s',text) #splits the text up by bullet points (1. , 2. , 3. , etc)\n",
    "    split_text = [x for x in split_text if x] #removes any empty splits\n",
    "    split_text = [x for x in split_text if x.isspace() is False] # removes any splits which are just whitespace.\n",
    "\n",
    "    # Check that I have extracted the correct number of answers.\n",
    "    # I know that these results should be answering the 7 questions defined in 'params' in the correct order.\n",
    "    # If the length doesn't match, the LLM question-answering has failed.\n",
    "    if len(split_text) == len(params):       \n",
    "        # assign the answers to the questions\n",
    "        for count, param in enumerate(params):\n",
    "            extracted_strings[param] = split_text[count]\n",
    "\n",
    "        # The bullet points contain text as well as numbers. This section extracts the first number given in each bullet point.\n",
    "        # Note that this action is not needed for dates, so it is skipped there.\n",
    "        extracted_numerical_values = {}\n",
    "        for key, value in extracted_strings.items():\n",
    "            if key != \"date\":\n",
    "                numerical_value = re.findall(r'(\\d+(?:\\.\\d+)?)', value) \n",
    "                #contains_million checks if the string contains M, m, or million (m or M only as individual letters)\n",
    "                contains_million = re.findall(r\"(\\d+(?:\\.\\d+)?)\\s*(?:m\\b|million)\",value, re.IGNORECASE)\n",
    "                if len(numerical_value) > 0:\n",
    "                    if len(contains_million)>0:\n",
    "                        extracted_numerical_values[key] = float(numerical_value[0])*1e6\n",
    "                    else:\n",
    "                        extracted_numerical_values[key] = float(numerical_value[0])\n",
    "                else:\n",
    "                    extracted_numerical_values[key] = np.nan\n",
    "            else:\n",
    "                extracted_numerical_values[key] = value\n",
    "\n",
    "        extracted_data.append(extracted_numerical_values)\n",
    "\n",
    "    else:\n",
    "        for count, param in enumerate(params):\n",
    "            extracted_strings[param] = np.nan\n",
    "        extracted_data.append(extracted_strings)\n",
    "        \n",
    "        \n",
    "df_extracted_data = pd.DataFrame(extracted_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958b43f6",
   "metadata": {},
   "source": [
    "Importing and cleaning reference data on the correct answers (to check accuracy of LLM). These data were collected by manually reading a small set of emails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed2205b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "comparison_data = pd.read_csv(\"example.csv\")\n",
    "\n",
    "# some columns have different names in the comparison_data set. This dictionary is used to rename them.\n",
    "column_names = {\n",
    "    'monthly recurring revenue' : 'MRR',\n",
    "    'burn rate' : 'burn_rate',\n",
    "    'annual recurring revenue' : 'ARR',\n",
    "}\n",
    "\n",
    "df_comparison_data = comparison_data[['monthly recurring revenue','runway','capital','FTEs','burn rate','annual recurring revenue']].replace(',','',regex=True)\n",
    "\n",
    "contains_million_mask = df_comparison_data.apply(lambda x: x.str.contains('m', case=False))\n",
    "\n",
    "df_comparison_data= df_comparison_data.replace('[^.0-9]', '', regex=True).astype(float,errors = 'ignore')\n",
    "\n",
    "df_comparison_data.replace('',np.nan,inplace= True)\n",
    "df_comparison_data = df_comparison_data.astype('float')\n",
    "\n",
    "df_comparison_data = df_comparison_data.mask(contains_million_mask,df_comparison_data*1e6)\n",
    "df_comparison_data.rename(columns=column_names, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54ca91e",
   "metadata": {},
   "source": [
    "Calculate the accuracy of the LLM data extraction by comparing to the manually-extracted results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b3d40c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "common_columns = list(set(df_extracted_data.columns).intersection(df_comparison_data.columns))\n",
    "\n",
    "avg_acc = 0.0\n",
    "for c in common_columns:\n",
    "    c_acc = accuracy_score(df_extracted_data[c].round(), df_comparison_data[c].round())\n",
    "    print(f'{c} accuracy: {c_acc:.2f}')\n",
    "    avg_acc += c_acc/len(common_columns)\n",
    "    \n",
    "print(f'Average accuracy: {avg_acc:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63fafa07",
   "metadata": {},
   "source": [
    "### OpenAI LLM analysis of emails<a name=\"openai_LLM\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd174cca",
   "metadata": {},
   "source": [
    "Using gpt-3.5-turbo model on Azure OpenAI service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85cab8f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_base = \"https://faculty-openai.openai.azure.com/\"\n",
    "openai.api_version = \"2023-05-15\"\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "\n",
    "results = {}\n",
    "\n",
    "# For each comment for the specified company, input the comment text as content, ask questions about it, and store the answer.\n",
    "for idx, row in df_comments[df_comments['trello_id']==company_trello_id].iterrows():\n",
    "    completion = openai.ChatCompletion.create(\n",
    "        engine=os.environ[\"OPENAI_ENGINE\"],\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": row['text']},\n",
    "            {\"role\": \"user\", \"content\": \"\"\"What are the current \n",
    "             total monthly revenue (which can also be called MRR), \n",
    "             monthly net cash burn, \n",
    "             number of employees employees, \n",
    "             cash in bank, \n",
    "             total annual revenue (which can also be called ARR),\n",
    "             runway,\n",
    "             and date on which this email was sent? \n",
    "             Output this information in the format of a python dictionary.\n",
    "             Do not include any additional comments.\n",
    "             If the information is not present, return \"unknown\".\n",
    "             \"\"\"}\n",
    "        ],\n",
    "        temperature = 0,\n",
    "    )\n",
    "    results[row['comment_id']] = completion.choices[0].message['content']\n",
    "    print(completion.choices[0].message['content'])\n",
    "\n",
    "# save the results\n",
    "with open(f'OpenAI_{company_name}.json', 'w') as f:\n",
    "    json.dump(results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398b7671",
   "metadata": {},
   "source": [
    "Importing results saved from running OpenAI gpt-3.5-turbo above and processing:\n",
    "- Using *ast* package to read output strings as a dictionary.\n",
    "- If data on cash is written with 'million' or 'M' or 'm', extract the number only and multiply it by 1,000,000.\n",
    "- Generally strip excess text and punctuation from results.\n",
    "\n",
    "*Note that the ingestion from OpenAI gpt-3.5-turbo is a lot easier as it can be asked to output the answer in the form of a dictionary without affecting the quality of its answers.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f5fc4c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(f'OpenAI_{company_name}.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "extracted_results = []\n",
    "\n",
    "reference_keys = ['MRR','burn_rate','FTEs','capital','ARR','runway','date']\n",
    "numerical_columns =['MRR','burn_rate','FTEs','capital','ARR','runway'] \n",
    "\n",
    "#iterating over the extracted data from all comments for a specified company\n",
    "for key in data:\n",
    "    # converting the result from a string to a dictionary\n",
    "    result = ast.literal_eval(data[key])\n",
    "    # OpenAI is not always consistent with its key naming when generating dictionaries.\n",
    "    # Here, we replace the OpenAI keys with the correct keys.\n",
    "    result_correct_keys = {}\n",
    "    for count, open_ai_key in enumerate(result.keys()):\n",
    "        result_correct_keys[reference_keys[count]] = result[open_ai_key]\n",
    "    extracted_results.append(result_correct_keys)\n",
    "\n",
    "# cleaning the extracted data to convert the strings to numeric data\n",
    "df = pd.DataFrame.from_dict(extracted_results).replace(',','',regex=True)\n",
    "contains_million_mask = df[numerical_columns].apply(lambda x: x.str.contains(r\"(\\d+(?:\\.\\d+)?)\\s*(?:m\\b|million)\", case=False))\n",
    "contains_million_mask.fillna(False,inplace=True)\n",
    "\n",
    "df[numerical_columns]= df[numerical_columns].replace('[^.0-9]', '', regex=True)\n",
    "df.replace('',np.nan,inplace= True)\n",
    "df[numerical_columns] = df[numerical_columns].astype('float')\n",
    "\n",
    "\n",
    "df[numerical_columns] = df[numerical_columns].mask(contains_million_mask,df[numerical_columns]*1e6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a1fb22",
   "metadata": {},
   "source": [
    "Importing reference data on the correct answers (to check accuracy of LLM). These data were collected by manually reading a small set of emails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9107aeeb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "comparison_data = pd.read_csv(\"example.csv\")\n",
    "column_names = {\n",
    "    'monthly recurring revenue' : 'MRR',\n",
    "    'burn rate' : 'burn_rate',\n",
    "    'annual recurring revenue' : 'ARR',\n",
    "}\n",
    "\n",
    "df_comparison_data = comparison_data[['monthly recurring revenue','runway','capital','FTEs','burn rate','annual recurring revenue']].replace(',','',regex=True)\n",
    "\n",
    "contains_million_mask = df_comparison_data.apply(lambda x: x.str.contains('m', case=False))\n",
    "\n",
    "df_comparison_data= df_comparison_data.replace('[^.0-9]', '', regex=True).astype(float,errors = 'ignore')\n",
    "\n",
    "df_comparison_data.replace('',np.nan,inplace= True)\n",
    "df_comparison_data = df_comparison_data.astype('float')\n",
    "\n",
    "df_comparison_data = df_comparison_data.mask(contains_million_mask,df_comparison_data*1e6)\n",
    "df_comparison_data.rename(columns=column_names, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018d4b48",
   "metadata": {},
   "source": [
    "Plotting the **OpenAI** LLM-extracted results along with the real data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809dd940",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.options.plotting.backend = \"plotly\"\n",
    "\n",
    "\n",
    "fig = make_subplots(rows=3, cols=2, subplot_titles=(\n",
    "    'Monthly revenue ($)',  \n",
    "    'Full-time employees',\n",
    "    'Runway (months)',\n",
    "    'Burn rate ($)',\n",
    "    'Capital ($)',\n",
    "    'ARR ($)',\n",
    "    ),\n",
    "    horizontal_spacing = 0.07,\n",
    "    vertical_spacing = 0.06,\n",
    ")\n",
    "\n",
    "plot_params = ['MRR', 'runway', 'capital', 'FTEs', 'burn_rate','ARR'] \n",
    "\n",
    "for count, param in enumerate(plot_params):\n",
    "    if count < 3:\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=df.index,\n",
    "            y=df[param],\n",
    "            name = 'Extracted by OpenAI',\n",
    "            mode = 'markers',\n",
    "            marker_color='rgba(9, 144, 185, 1)',\n",
    "            legendgroup=\"LLM\",\n",
    "            showlegend = count == 0,\n",
    "        ),\n",
    "              row=count+1, col=1)\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=df_comparison_data.index,\n",
    "            y=df_comparison_data[param],\n",
    "            name = 'Manually labelled', \n",
    "            mode = 'markers',\n",
    "            marker_line_color='rgba(224, 13, 45, 1)',\n",
    "            marker_symbol = 'x-thin',\n",
    "            marker_line_width=1,\n",
    "            marker_size = 10,\n",
    "            legendgroup=\"actual\",\n",
    "            showlegend = count == 0,\n",
    "        ),\n",
    "              row=count+1, col=1)\n",
    "    else:\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=df.index, \n",
    "            y=df[param],\n",
    "            name = 'OpenAI', \n",
    "            mode = 'markers',\n",
    "            marker_color='rgba(9, 144, 185, 1)',\n",
    "            legendgroup=\"LLM\",\n",
    "            showlegend = count == 0,\n",
    "        ),\n",
    "              row=count-2, col=2)\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=df_comparison_data.index, \n",
    "            y=df_comparison_data[param],\n",
    "            name = 'Reference', \n",
    "            mode = 'markers',\n",
    "            marker_line_color='rgba(224, 13, 45, 1)',\n",
    "            marker_symbol = 'x-thin',\n",
    "            marker_line_width=1,\n",
    "            marker_size = 10,\n",
    "            legendgroup=\"actual\",\n",
    "            showlegend = count == 0,\n",
    "        ),\n",
    "              row=count-2, col=2)\n",
    "        \n",
    "\n",
    "fig.update_layout(legend=dict(\n",
    "        orientation=\"h\",\n",
    "        yanchor=\"bottom\",\n",
    "        y=1.05,\n",
    "        xanchor=\"left\",\n",
    "        x=0.21,\n",
    "        font=dict(size= 20)\n",
    "    ),\n",
    "    width=1000,\n",
    "    height=800,\n",
    "    font=dict(\n",
    "        size = 18\n",
    "    ),\n",
    "                 \n",
    ")\n",
    "\n",
    "fig.update_xaxes(visible=False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cff6764",
   "metadata": {},
   "source": [
    "Calculating the accuracy of OpenAI data extraction by comparing its results to manually-extracted data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78442d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "common_columns = list(set(df.columns).intersection(df_comparison_data.columns))\n",
    "\n",
    "for c in common_columns:\n",
    "    c_acc = accuracy_score(df[c].round(), df_comparison_data[c].round())\n",
    "    print(f'{c} accuracy: {c_acc:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d492ed",
   "metadata": {},
   "source": [
    "### Comparison of LLM results<a name=\"LLM_comparison\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc67c9e",
   "metadata": {},
   "source": [
    "Summary of accuracy from a range of models (example data copied from accuracies generated using above code):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e0ad23",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "models = ['OpenAI','Falcon 40B','Vicuna 33B','RedPajama']\n",
    "\n",
    "results = {\n",
    "    'OpenAI' : {\n",
    "        'annual<br>revenue' : 1,\n",
    "        'burn<br>rate' : 1,\n",
    "        'capital' : 1,\n",
    "        'employees' : 1,\n",
    "        'monthly<br>revenue' : 1,\n",
    "        'runway' : 1,\n",
    "    },\n",
    "    'Falcon 40B' : {\n",
    "        'annual<br>revenue' : 0.78,\n",
    "        'burn<br>rate' : 1,\n",
    "        'capital' : 1,\n",
    "        'employees' : 0.39,\n",
    "        'monthly<br>revenue' : 0.91,\n",
    "        'runway' : 1,\n",
    "    },\n",
    "    'Vicuna 33B' : {\n",
    "        'annual<br>revenue' : 0.78,\n",
    "        'burn<br>rate' : 1,\n",
    "        'capital' : 1,\n",
    "        'employees' : 0.91,\n",
    "        'monthly<br>revenue' : 0.17,\n",
    "        'runway' : 1,\n",
    "    },\n",
    "    'RedPajama' : {\n",
    "        'annual<br>revenue' : 0.22,\n",
    "        'burn<br>rate' : 0.7,\n",
    "        'capital' : 0.7,\n",
    "        'employees' : 0.09,\n",
    "        'monthly<br>revenue' : 0.61,\n",
    "        'runway' : 0.78,\n",
    "    },\n",
    "}\n",
    "\n",
    "df_summary = pd.DataFrame.from_dict(results)\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "\n",
    "\n",
    "fig.add_trace(go.Scatter(x=df_summary.index, y=df_summary['Falcon 40B'],\n",
    "                    mode='markers',\n",
    "                    marker_size = 15,\n",
    "                    marker_line_color='rgba(9, 144, 185, 1)',\n",
    "                    marker_line_width=1.5,\n",
    "                    marker_symbol = 'x-thin',\n",
    "                    name = 'Falcon 40B',\n",
    "                    legendgrouptitle_text=\"Open source\",\n",
    "                    legendgroup = 'Open source'\n",
    "                        ))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=df_summary.index, y=df_summary['Vicuna 33B'],\n",
    "                    mode='markers',\n",
    "                    marker_size = 15,\n",
    "                    marker_line_color='rgba(9, 144, 185, 1)',\n",
    "                    marker_line_width=1.5,\n",
    "                    marker_symbol = 'cross-thin',\n",
    "                    name = 'Vicuna 33B',\n",
    "                    legendgrouptitle_text=\"Open source\",\n",
    "                    legendgroup = 'Open source'\n",
    "                        ))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=df_summary.index, y=df_summary['RedPajama'],\n",
    "                    mode='markers',\n",
    "                    marker_size = 10,\n",
    "                    marker_color='rgba(9, 144, 185, 1)',\n",
    "                    marker_line_width=1.5,\n",
    "                    marker_symbol = 'circle-open',\n",
    "                    name = 'RedPajama<br>INCITE-7B',\n",
    "                    legendgrouptitle_text=\"Open source\",\n",
    "                    legendgroup = 'Open source'\n",
    "                        ))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=df_summary.index, y=df_summary['OpenAI'],\n",
    "                    mode='markers',\n",
    "                    marker_size = 10,\n",
    "                    marker_color = 'rgba(235, 118, 12, 1)',\n",
    "                    name = 'OpenAI<br>gpt-3.5-turbo',\n",
    "                    legendgrouptitle_text=\"Proprietary\",\n",
    "                    legendgroup = 'Proprietary'\n",
    "                        ))\n",
    "\n",
    "\n",
    "fig.update_layout(\n",
    "    autosize=False,\n",
    "    yaxis_title=\"Accuracy\",\n",
    "    xaxis_title = 'Extracted metrics',\n",
    "#     legend_title=\"Model\",\n",
    "    font = dict(\n",
    "        size = 14\n",
    "    ),\n",
    "    legend_traceorder='grouped+reversed'\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "fig.show()\n",
    "fig.write_image(\"LLM_results_summary.svg\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
